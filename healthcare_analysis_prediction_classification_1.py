# -*- coding: utf-8 -*-
"""Copy of Healthcare analysis prediction classification 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZExzMyN01C5nTDo2_SnxmZ18mSEApjC2

Data Cleaning & Preprocessing

Step 1:  Import Libraries & Load the Dataset1.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# For preprocessing and modeling
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score

import pandas as pd # Import the pandas library

df = pd.read_csv("//content/healthcare_dataset.csv")

df.head()

"""Step 2:  Basic Info & Null Check"""

# Basic structure
print(df.shape)
print(df.dtypes)
df.info()

# Check for nulls
df.isnull().sum()

"""Step 3:  Data Cleaning

 Convert date columns to datetime
"""

df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])

""" Remove any weird outliers or negative values

"""

# Check negative billing or length of stay
df = df[df['Billing Amount'] >= 0]
df = df[df['Age'] >= 0]

"""Step 4:  Feature Engineering

 Length of Stay
"""

df['Length of Stay'] = (df['Discharge Date'] - df['Date of Admission']).dt.days
df = df[df['Length of Stay'] >= 0]  # remove inconsistent dates

""" Admission Month & Year"""

df['Admission Month'] = df['Date of Admission'].dt.month
df['Admission Year'] = df['Date of Admission'].dt.year

""" Extract Test Result Category (Optional Parsing Example)"""

# Placeholder example: categorize as "Positive", "Negative", or "Inconclusive"
df['Test Category'] = df['Test Results'].apply(lambda x: 'Positive' if 'positive' in x.lower() else 'Negative' if 'negative' in x.lower() else 'Other')

"""Step 5:  Encode Categorical Variables

Letâ€™s check them first:
"""

df.select_dtypes(include='object').nunique()

"""You can encode:


Gender, Blood Type, Admission Type, Medical Condition, Doctor, Hospital, Insurance Provider, Medication, Test Category


Using LabelEncoder (for simplicity):
"""

# Make sure you import LabelEncoder if it hasn't been imported previously
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for col in ['Gender', 'Blood Type', 'Admission Type', 'Medical Condition', 'Doctor',
            'Hospital', 'Insurance Provider', 'Medication', 'Test Category', 'Test Results']:
    df[col] = le.fit_transform(df[col])

"""Part 2: Exploratory Data Analysis (EDA)

We'll break this into Univariate, Bivariate, and Multivariate analyses using plots and statistics.

ðŸ”¹ Step 1: Univariate Analysis
"""



"""1. Gender Count

"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt # Import the matplotlib library and assign it to the alias 'plt'


# For preprocessing and modeling
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score

# %%
import pandas as pd # Import the pandas library

df = pd.read_csv("//content/healthcare_dataset.csv")
# %%
df.head()
# %% [markdown]
# Step 2:  Basic Info & Null Check
# %%
# Basic structure
print(df.shape)
print(df.dtypes)
df.info()

# Check for nulls
df.isnull().sum()

# %% [markdown]
# Step 3:  Data Cleaning
#
#  Convert date columns to datetime
# %%
df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])

# %% [markdown]
#  Remove any weird outliers or negative values
#
# %%
# Check negative billing or length of stay
df = df[df['Billing Amount'] >= 0]
df = df[df['Age'] >= 0]

# %% [markdown]
# Step 4:  Feature Engineering
#
#  Length of Stay
# %%
df['Length of Stay'] = (df['Discharge Date'] - df['Date of Admission']).dt.days
df = df[df['Length of Stay'] >= 0]  # remove inconsistent dates

# %% [markdown]
#  Admission Month & Year
# %%
df['Admission Month'] = df['Date of Admission'].dt.month
df['Admission Year'] = df['Date of Admission'].dt.year

# %% [markdown]
#  Extract Test Result Category (Optional Parsing Example)
# %%
# Placeholder example: categorize as "Positive", "Negative", or "Inconclusive"
df['Test Category'] = df['Test Results'].apply(lambda x: 'Positive' if 'positive' in x.lower() else 'Negative' if 'negative' in x.lower() else 'Other')

# %% [markdown]
# Step 5:  Encode Categorical Variables
#
# Letâ€™s check them first:
# %%
df.select_dtypes(include='object').nunique()

# %% [markdown]
# You can encode:
#
#
# Gender, Blood Type, Admission Type, Medical Condition, Doctor, Hospital, Insurance Provider, Medication, Test Category
#
#
# Using LabelEncoder (for simplicity):
# %%
# Make sure you import LabelEncoder if it hasn't been imported previously
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
for col in ['Gender', 'Blood Type', 'Admission Type', 'Medical Condition', 'Doctor',
            'Hospital', 'Insurance Provider', 'Medication', 'Test Category', 'Test Results']:
    df[col] = le.fit_transform(df[col])
# %% [markdown]
#  Part 2: Exploratory Data Analysis (EDA)
#
# We'll break this into Univariate, Bivariate, and Multivariate analyses using plots and statistics.
# %% [markdown]
# ðŸ”¹ Step 1: Univariate Analysis
#
# 1. Age Distribution
# %%

# %%
plt.figure(figsize=(10, 5)) # 'plt' is now defined and can be used
sns.histplot(df['Age'], bins=30, kde=True, color='skyblue')
plt.title("Age Distribution")
plt.xlabel("Age")
plt.ylabel("Count")
plt.show()

"""2. Billing Amount Distribution"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt # Import the matplotlib library and assign it to the alias 'plt'


# For preprocessing and modeling
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score

# %%
import pandas as pd # Import the pandas library

df = pd.read_csv("//content/healthcare_dataset.csv")
# %%
df.head()
# %% [markdown]
# Step 2:  Basic Info & Null Check
# %%
# Basic structure
print(df.shape)
print(df.dtypes)
df.info()

# Check for nulls
df.isnull().sum()

# %% [markdown]
# Step 3:  Data Cleaning
#
#  Convert date columns to datetime
# %%
df['Date of Admission'] = pd.to_datetime(df['Date of Admission'])
df['Discharge Date'] = pd.to_datetime(df['Discharge Date'])

# %% [markdown]
#  Remove any weird outliers or negative values
#
# %%
# Check negative billing or length of stay
df = df[df['Billing Amount'] >= 0]
df = df[df['Age'] >= 0]

# %% [markdown]
# Step 4:  Feature Engineering
#
#  Length of Stay
# %%
df['Length of Stay'] = (df['Discharge Date'] - df['Date of Admission']).dt.days
df = df[df['Length of Stay'] >= 0]  # remove inconsistent dates

# %% [markdown]
#  Admission Month & Year
# %%

"""4. Admission Type"""

sns.countplot(y='Admission Type', data=df, order=df['Admission Type'].value_counts().index)
plt.title("Admission Types Count")
plt.xlabel("Count")
plt.ylabel("Admission Type")
plt.show()

"""ðŸ”¹ Step 2: Bivariate Analysis

1. Age vs Billing Amount
"""

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Age', y='Billing Amount', data=df, alpha=0.5)
plt.title("Age vs Billing Amount")
plt.show()

"""2. Gender vs Billing Amount"""

plt.figure(figsize=(8, 6))
sns.boxplot(x='Gender', y='Billing Amount', data=df)
plt.title("Billing Amount by Gender")
plt.show()

"""3. Admission Type vs Length of Stay"""

plt.figure(figsize=(10, 6))
sns.boxplot(x='Admission Type', y='Length of Stay', data=df)
plt.title("Length of Stay by Admission Type")
plt.xticks(rotation=45)
plt.show()

"""ðŸ”¹ Step 3: Multivariate Analysis

1. Heatmap of Correlations
"""

plt.figure(figsize=(12, 10))
corr = df.corr(numeric_only=True)
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

"""2. Pairplot"""

subset = df[['Age', 'Billing Amount', 'Length of Stay']]
sns.pairplot(subset)
plt.show()

"""Part 3: Predictive Modeling

We'll divide this into two sections:

Regression â€“ Predicting Billing Amount

Classification â€“ Predicting Admission Type

Step 1: Feature Selection
"""

# Drop non-numeric or less useful for this task
features = df.drop(columns=['Name', 'Billing Amount', 'Date of Admission', 'Discharge Date'])

X_reg = features
y_reg = df['Billing Amount']

categorical_cols = X_reg.select_dtypes(include='object').columns.tolist()
print("Categorical Columns:", categorical_cols)

from sklearn.preprocessing import LabelEncoder

X_encoded = X_reg.copy()
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))
    label_encoders[col] = le

""" Step 2: Train-Test Split"""

from sklearn.model_selection import train_test_split

X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_encoded, y_reg, test_size=0.2, random_state=42)

""" Step 3: Model Training (Random Forest Regressor)"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)
rf_reg.fit(X_train_reg, y_train_reg)

y_pred_reg = rf_reg.predict(X_test_reg)

""" Step 4: Evaluation"""

mae = mean_absolute_error(y_test_reg, y_pred_reg)
rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))

print("MAE:", mae)
print("RMSE:", rmse)

""" Section 2: Classification â€“ Predicting Admission Type

 Target Variable: Admission Type
"""

# Admission Type was label-encoded already in preprocessing
X_clf = df.drop(columns=['Name', 'Admission Type', 'Date of Admission', 'Discharge Date', 'Billing Amount'])

y_clf = df['Admission Type']

""" Step 2: Train-Test Split"""

X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)

""" Step 3: Model Training (Random Forest Classifier)"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_clf.fit(X_train_clf, y_train_clf)

y_pred_clf = rf_clf.predict(X_test_clf)

""" Step 4: Evaluation"""

print("Classification Report:\n", classification_report(y_test_clf, y_pred_clf))

cm = confusion_matrix(y_test_clf, y_pred_clf)

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

"""Weâ€™ll implement and compare:


ðŸ’  Linear Regression and XGBoost Regressor for Billing Amount


ðŸ”· Logistic Regression and XGBoost Classifier for Admission Type

1. Linear Regression
"""

from sklearn.linear_model import LinearRegression

linreg = LinearRegression()
linreg.fit(X_train_reg, y_train_reg)

y_pred_lin = linreg.predict(X_test_reg)

mae_lin = mean_absolute_error(y_test_reg, y_pred_lin)
rmse_lin = np.sqrt(mean_squared_error(y_test_reg, y_pred_lin))

print("Linear Regression MAE:", mae_lin)
print("Linear Regression RMSE:", rmse_lin)

"""2. XGBoost Regressor"""

from xgboost import XGBRegressor

xgb_reg = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_reg.fit(X_train_reg, y_train_reg)

y_pred_xgb = xgb_reg.predict(X_test_reg)

mae_xgb = mean_absolute_error(y_test_reg, y_pred_xgb)
rmse_xgb = np.sqrt(mean_squared_error(y_test_reg, y_pred_xgb))

print("XGBoost Regression MAE:", mae_xgb)
print("XGBoost Regression RMSE:", rmse_xgb)

"""ðŸ”·  Classification Models â€“ Predicting Admission Type

1. Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression(max_iter=1000)
logreg.fit(X_train_clf, y_train_clf)

y_pred_log = logreg.predict(X_test_clf)
print("Logistic Regression:\n", classification_report(y_test_clf, y_pred_log))

"""2. XGBoost Classifier"""

from xgboost import XGBClassifier

xgb_clf = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, use_label_encoder=False, eval_metric='mlogloss')
xgb_clf.fit(X_train_clf, y_train_clf)

y_pred_xgb_clf = xgb_clf.predict(X_test_clf)

print("XGBoost Classifier:\n", classification_report(y_test_clf, y_pred_xgb_clf))

""" Model Comparison Summary"""

print(" Regression Metrics:")
print(f"Linear Regression: MAE={mae_lin:.2f}, RMSE={rmse_lin:.2f}")
print(f"Random Forest: MAE={mae:.2f}, RMSE={rmse:.2f}")
print(f"XGBoost: MAE={mae_xgb:.2f}, RMSE={rmse_xgb:.2f}")

print("\n Classification Accuracy Scores:")
print(f"Logistic Regression: {accuracy_score(y_test_clf, y_pred_log):.4f}")
print(f"Random Forest: {accuracy_score(y_test_clf, y_pred_clf):.4f}")
print(f"XGBoost: {accuracy_score(y_test_clf, y_pred_xgb_clf):.4f}")

""" Part 4: Hyperparameter Tuning

We'll tune:

 RandomForestRegressor

 RandomForestClassifier

 XGBoostClassifier

Weâ€™ll use GridSearchCV for simplicity
"""

from sklearn.model_selection import GridSearchCV

param_grid_rf_reg = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
}

grid_rf_reg = GridSearchCV(RandomForestRegressor(random_state=42),
                           param_grid_rf_reg,
                           cv=3,
                           scoring='neg_root_mean_squared_error',
                           verbose=1,
                           n_jobs=-1)

grid_rf_reg.fit(X_train_reg, y_train_reg)

print("Best Params:", grid_rf_reg.best_params_)
print("Best RMSE:", -grid_rf_reg.best_score_)

"""B. Random Forest Classifier â€“ Tuning for Admission Type"""

param_grid_rf_clf = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
}

grid_rf_clf = GridSearchCV(RandomForestClassifier(random_state=42),
                           param_grid_rf_clf,
                           cv=3,
                           scoring='accuracy',
                           verbose=1,
                           n_jobs=-1)

grid_rf_clf.fit(X_train_clf, y_train_clf)

print("Best Params:", grid_rf_clf.best_params_)
print("Best Accuracy:", grid_rf_clf.best_score_)

"""C. XGBoost Classifier â€“ Tuning for Admission Type"""

param_grid_xgb = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0],
}

grid_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),
                        param_grid_xgb,
                        cv=3,
                        scoring='accuracy',
                        verbose=1,
                        n_jobs=-1)

grid_xgb.fit(X_train_clf, y_train_clf)

print("Best Params:", grid_xgb.best_params_)
print("Best Accuracy:", grid_xgb.best_score_)